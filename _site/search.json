[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Yo whats up it’s me Porter!! (That’s my cousin Josh)"
  },
  {
    "objectID": "articles/coding/cpp_vs_python.html",
    "href": "articles/coding/cpp_vs_python.html",
    "title": "C++ vs Python For Machine Learning",
    "section": "",
    "text": "Both C++ and Python are powerful, capable languages. Lets explore some of their pros and cons for machine learning."
  },
  {
    "objectID": "articles/coding/cpp_vs_python.html#background",
    "href": "articles/coding/cpp_vs_python.html#background",
    "title": "C++ vs Python For Machine Learning",
    "section": "",
    "text": "Both C++ and Python are powerful, capable languages. Lets explore some of their pros and cons for machine learning."
  },
  {
    "objectID": "articles/math/nns.html",
    "href": "articles/math/nns.html",
    "title": "Deep Learning: The Future of Intelligence",
    "section": "",
    "text": "Deep learning is a type of artifitial intelligence model that uses linear algebra and multivariable calculus to learn the relationship between an set of inputs and a set of outputs. A deep learning model is a type of neural network that involves more than 1 hidden layer.\n\n\n\n\nTransformations, mappings, vector and matrix multiplication and other operations from linear algebra\nPartial derivatives and their implications from multivariable calculus"
  },
  {
    "objectID": "articles/math/nns.html#prerequisites",
    "href": "articles/math/nns.html#prerequisites",
    "title": "Deep Learning: The Future of Intelligence",
    "section": "",
    "text": "Transformations, mappings, vector and matrix multiplication and other operations from linear algebra\nPartial derivatives and their implications from multivariable calculus"
  },
  {
    "objectID": "articles/math/nns.html#summary",
    "href": "articles/math/nns.html#summary",
    "title": "Deep Learning: The Future of Intelligence",
    "section": "Summary",
    "text": "Summary\nTo summarise the forward propogations process…\n$$ T_1() = (W_1) = \nT_2() = W_2 = $$\nAre the linear mappings represented by weight matrices that map the input vector to the hidden vector to the output vector. Now we will study the process of optimizing the weights to minimize the loss function \\(\\mathcal{L}\\)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Stretch",
    "section": "",
    "text": "Cool data science blog. Currently in its early stages please be patient."
  }
]