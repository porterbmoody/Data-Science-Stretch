[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Yo whats up it’s me Porter!! (That’s my cousin Josh)"
  },
  {
    "objectID": "articles/coding/cpp_vs_python.html",
    "href": "articles/coding/cpp_vs_python.html",
    "title": "C++ vs Python For Machine Learning",
    "section": "",
    "text": "Both C++ and Python are powerful, capable languages. Lets explore some of their pros and cons for machine learning."
  },
  {
    "objectID": "articles/coding/cpp_vs_python.html#background",
    "href": "articles/coding/cpp_vs_python.html#background",
    "title": "C++ vs Python For Machine Learning",
    "section": "",
    "text": "Both C++ and Python are powerful, capable languages. Lets explore some of their pros and cons for machine learning."
  },
  {
    "objectID": "articles/math/nns.html",
    "href": "articles/math/nns.html",
    "title": "Linear Algebra and Multivariable Calculus is All you Need",
    "section": "",
    "text": "A Neural Network is a type of artifitial intelligence model that uses linear algebra and multivariable calculus to learn and store the relationship between an input vector and an output vector that minimizes the loss function. Here is lined out the mathematics for a single layer neural network.\nNote: DNN’s (Deep Neural Networks) or neural networks with more than 1 hidden layer will be covered in another article."
  },
  {
    "objectID": "articles/math/nns.html#background",
    "href": "articles/math/nns.html#background",
    "title": "Linear Algebra and Multivariable Calculus is All you Need",
    "section": "",
    "text": "A Neural Network is a type of artifitial intelligence model that uses linear algebra and multivariable calculus to learn and store the relationship between an input vector and an output vector that minimizes the loss function. Here is lined out the mathematics for a single layer neural network.\nNote: DNN’s (Deep Neural Networks) or neural networks with more than 1 hidden layer will be covered in another article."
  },
  {
    "objectID": "articles/math/nns.html#prerequisites",
    "href": "articles/math/nns.html#prerequisites",
    "title": "Linear Algebra and Multivariable Calculus is All you Need",
    "section": "Prerequisites:",
    "text": "Prerequisites:\n\nTransformations, mappings, vector and matrix multiplication and other operations from linear algebra\nPartial derivatives and their implications from multivariable calculus"
  },
  {
    "objectID": "articles/math/nns.html#forward-propagation",
    "href": "articles/math/nns.html#forward-propagation",
    "title": "Linear Algebra and Multivariable Calculus is All you Need",
    "section": "Forward Propagation",
    "text": "Forward Propagation\nThe input, hidden, and output layers are represented by the following vectors, respectively.\n\\[\n\\vec{x} =\n\\begin{bmatrix}\n    x_1\\\\\n    x_2\\\\\n    \\vdots\\\\\n    x_m\n\\end{bmatrix} \\in \\mathbb{R^m}\n,\n\\vec{h} =\n\\begin{bmatrix}\n    h_1\\\\\n    h_2\\\\\n    \\vdots\\\\\n    h_h\n\\end{bmatrix} \\in \\mathbb{R^h}\n,\n\\vec{y} =\n\\begin{bmatrix}\n    y_1\\\\\n    y_2\\\\\n    \\vdots\\\\\n    y_n\n\\end{bmatrix} \\in \\mathbb{R^n}\n\\]\nLet \\(T_1: \\mathbb{R^m} \\rightarrow \\mathbb{R^h}\\) be a linear mapping. \\(T_1\\) maps the input layer vector \\(\\vec{x} \\in \\mathbb{R^m}\\) to the hidden layer vector \\(\\vec{h} \\in \\mathbb{R^h}\\). We will use the sigmoid (\\(\\sigma\\)) activation function which has a domain of \\((-\\infty,\\infty)\\) and range of \\((-1,1)\\). Therefore the the codomain of \\(T_1\\) is \\((-1,1)\\).\n$$ \\[\\begin{gather}\nW_1 =\n    \\begin{bmatrix}\n    w_{1,1} & w_{1,2} & \\ldots & w_{1,m}\\\\\n    w_{2,1} & w_{2,2} & \\ldots & w_{2,m}\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n    w_{h,1} & w_{h,2} & \\ldots & w_{h,m}\\\\\n    \\end{bmatrix}\n\\newline\nT_1(\\vec{x}) = \\sigma(W_1\\vec{x}) = \\vec{s}\n\n\\end{gather}\\] $$\nAnd let \\(T_2: \\mathbb{R^h} \\rightarrow \\mathbb{R^n}\\) be a linear mapping. \\(T_2\\) maps the hidden layer vector \\(\\vec{h} \\in \\mathbb{R^h}\\) to the output layer vector \\(\\vec{y} \\in \\mathbb{R^n}\\).\n$$ \\[\\begin{gather}\nW_2 = \\begin{bmatrix}\n    w_{1,1} & w_{1,2} & \\ldots & w_{1,h}\\\\\n    w_{2,1} & w_{2,2} & \\ldots & w_{2,h}\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n    w_{n,1} & w_{n,2} & \\ldots & w_{n,h}\\\\\n    \\end{bmatrix}\n\n\\newline\nT_2(\\vec{h}) = W_2\\vec{h} = \\vec{y}\n\n\\end{gather}\\] $$\nThese weight matrices are initally composed of randomly generated scalars, but during backpropagation the values of the true weights are esitmated. The hypothesis of the neural network is that the true weight matrices for the given transformation explains the true relationship between the input and output vectors.\nTo summarise the forward propogations process…\n$$ T_1() = (W_1) = T_2() = W_2 = \n$$"
  },
  {
    "objectID": "articles/math/nns.html#back-propgation",
    "href": "articles/math/nns.html#back-propgation",
    "title": "Linear Algebra and Multivariable Calculus is All you Need",
    "section": "Back Propgation",
    "text": "Back Propgation\nIf you haven’t already been mezmerised, get ready for the real magic show: back progagation. It is the process of adjusting the weights and biases in direction of the local minimun of the loss function with respect to each individual weight as computed by the partial derivative of the loss function with respect to the given weight.\nRemember, the derivative"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Stretch",
    "section": "",
    "text": "Cool data science blog. Currently in its early stages please be patient."
  }
]